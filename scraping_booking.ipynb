{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Mining and Natural Language Processing\n",
    "## Project 1 Booking \n",
    "\n",
    "by Natalia BeltrÃ¡n, Harry Morley, Xi Cheng "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Future Event "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This mini research project aims to investigate the effect of the SONAR Festival has on the rental prices in Barcelona. The SONAR Festival is a prominent music festival that is held annually in June, and attracks a large influx of visitors. The festival takes place during the week of the 12th to the 16th of June. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Period & Second City Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen time periods for data collection are the weeks of the 5th to 9th and 12th to 16th of June. This selection allows for a comparative analysis before and during the SONAR Festival in Barcelona. By contrasting these two weeks, we aim to capture any discernible shifts in rental prices that could be associated with the event, offering a comprehensive understanding of its impact. Alicante was selected as the second city for its comparable characteristics to Barcelona. Both cities share a coastal setting, vibrant cultural scenes, and historical attractions offering a similar appeal to tourists. By examining rental prices in Alicante during the same time frame, we aim to discern whether the observed trends in Barcelona are event-specific or part of a broader pattern in cites with similar profiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our research relies on a well-structured pipeline to systematically collect data on the rental prices during significant events. This automated framework ensures efficiency and reliability in extracting information from the Booking platform. In this section we detail the key steps of the pipeline, specifically focusing on Barcelona during the weeks before and during the SONAR Festival. \n",
    "\n",
    "- Initialize:\n",
    "    - Initializing the web scraping pipeline by selecting the preferred browser (Firefox) using Selenium. \n",
    "- Open the Booking browser \n",
    "- Handling Cookies: \n",
    "    - Mechanism implemented to handle cookie pop-ups that may appear during web browsing. \n",
    "- Selecting Travel Destination: \n",
    "    - Navigate to the desired travel section on the booking platform and input the desired city for analysis. \n",
    "- Date Selection: \n",
    "    - Open the calendar section to select the desired date range for analysis. This includes functions to interact with the calendar, allowing for the specification of both start and end dates. \n",
    "- Initiate the hotel search process: \n",
    "    - Prompts the platform to fetch relevant accommodation options. This marks the beginning of the data retrieval phase. \n",
    "- Hotel Pages: \n",
    "    - Count the total number of pages containing hotel options, allows the pipeline to adapt to varying search result pages and ensures comprehensive data collection. \n",
    "- Load XPaths: \n",
    "    - Loads all the necessary XPaths for a systematic and efficient way to identify and interact with specific elements of the web page, allowing for accurate data extraction from various hotel listings. \n",
    "- Hotel Data Extraction: \n",
    "    - Retrieve information such as hotel names, ratings, and prices. The Booking page was broken down into different sections for each hotel listing in order to more seamlessly and efficiently extract the hotel information from all the relevant pages. \n",
    "- Description Extraction: \n",
    "    - Extract detailed descriptions of each hotel. \n",
    "- Save as CSV: \n",
    "    - Combine extracted hotel data and descriptions, and save the consolidated information in a CSV file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scrape date, room price, hotel name, and hotel description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, StaleElementReferenceException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import statsmodels.api as sm\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# Headers\n",
    "#headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'}\n",
    " \n",
    "# Go get geckodriver from : https://github.com/mozilla/geckodriver/releases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browsing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffx_preferences(dfolder, download=False):\n",
    "    '''\n",
    "    Sets the preferences of the firefox browser: download path.\n",
    "    '''\n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    # set download folder:\n",
    "    profile.set_preference(\"browser.download.dir\", dfolder)\n",
    "    profile.set_preference(\"browser.download.folderList\", 2)\n",
    "    profile.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "    profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\",\n",
    "                           \"application/msword,application/rtf, application/csv,text/csv,image/png ,image/jpeg, application/pdf, text/html,text/plain,application/octet-stream\")\n",
    "    \n",
    "\n",
    "    # this allows to download pdfs automatically\n",
    "    if download:\n",
    "        profile.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/pdf,application/x-pdf\")\n",
    "        profile.set_preference(\"pdfjs.disabled\", True)\n",
    "\n",
    "    options = Options()\n",
    "    options.profile = profile\n",
    "    return options\n",
    "\n",
    "\n",
    "def start_up(link, dfolder, geko_path,donwload=True):\n",
    "    os.makedirs(dfolder, exist_ok=True)\n",
    "\n",
    "    options = ffx_preferences(dfolder,donwload)\n",
    "    service = Service(geko_path)\n",
    "    browser = webdriver.Firefox(service=service, options=options)\n",
    "    # Enter the website address here\n",
    "    browser.get(link)\n",
    "    time.sleep(5)  # Adjust sleep time as needed\n",
    "    return browser\n",
    "\n",
    "\n",
    "def check_and_click(browser, xpath, type):\n",
    "    '''\n",
    "    Function that checks whether the object is clickable and, if so, clicks on\n",
    "    it. If not, waits one second and tries again.\n",
    "    '''\n",
    "    ck = False\n",
    "    ss = 0\n",
    "    while ck == False:\n",
    "        ck = check_obscures(browser, xpath, type)\n",
    "        time.sleep(1)\n",
    "        ss += 1\n",
    "        if ss == 15:\n",
    "            ck = True\n",
    "            \n",
    "\n",
    "def check_obscures(browser, xpath, type):\n",
    "    '''\n",
    "    Function that checks whether the object is being \"obscured\" by any element so\n",
    "    that it is not clickable. Important: if True, the object is going to be clicked!\n",
    "    '''\n",
    "    try:\n",
    "        if type == \"xpath\":\n",
    "            browser.find_element('xpath',xpath).click()\n",
    "        elif type == \"id\":\n",
    "            browser.find_element('id',xpath).click()\n",
    "        elif type == \"css\":\n",
    "            browser.find_element('css selector',xpath).click()\n",
    "        elif type == \"class\":\n",
    "            browser.find_element('class name',xpath).click()\n",
    "        elif type == \"link\":\n",
    "            browser.find_element('link text',xpath).click()\n",
    "    except (ElementClickInterceptedException, NoSuchElementException, StaleElementReferenceException) as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the Booking.com website \n",
    "\n",
    "dfolder='/Users/nataliabeltran/Downloads/BSE_courses/Term_2_courses/22DM014_Introduction_to_Text_Mining_NLP/ta_session_text_mining/booking_assignment'\n",
    "geko_path='/Users/nataliabeltran/Downloads/geckodriver'\n",
    "link='https://www.booking.com/index.es.html'\n",
    "\n",
    "\n",
    "browser=start_up(dfolder=dfolder,link=link,geko_path=geko_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reject the cookies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejects the cookies after logging into the Booking website\n",
    "x_path_cookies = '//button[@id=\"onetrust-reject-all-handler\"]'\n",
    "check_and_click(browser, x_path_cookies, 'xpath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the city to travel section\n",
    "browser.find_element(by='xpath',value='//input[@id=\":re:\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows us to input a city to scrape hotels for \n",
    "place = input('Where do you want to go?')\n",
    "search1 = browser.find_element(by='xpath',value='//*[@id=\":re:\"]')\n",
    "search1.send_keys(place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the calendar section \n",
    "css='button.ebbedaf8ac:nth-child(2) > span:nth-child(1)'\n",
    "browser.find_element('css selector',css).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets all the dates of the calendar\n",
    "path='//div[@id=\"calendar-searchboxdatepicker\"]//table[@class=\"eb03f3f27f\"]//tbody//td[@class=\"b80d5adb18\"]//span[@class=\"cf06f772fa\"]'\n",
    "\n",
    "dates = browser.find_elements('xpath',path)\n",
    "\n",
    "\n",
    "for date in dates:\n",
    "    print(date.get_attribute(\"data-date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_element(browser, xpath, num_clicks=1):\n",
    "    \"\"\"\n",
    "    Clicks the arrow element that is identified by the given XPath multiple times.\n",
    "\n",
    "    Parameters:\n",
    "    - browser (WebDriver): The browser instance to interact with.\n",
    "    - xpath (str): XPath of the element to click (arrow).\n",
    "    - num_clicks (int): Number of times to click the element. Default is 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    for _ in range(num_clicks):\n",
    "        browser.find_element('xpath', xpath).click()\n",
    "\n",
    "def select_month(browser, xpath_button, num_month):\n",
    "    \"\"\"\n",
    "    Selects a month by clicking the arrow element identified by the given XPath a certain amount of times.\n",
    "\n",
    "    Parameters:\n",
    "    - browser (WebDriver): The browser instance to interact with.\n",
    "    - xpath_button (str): XPath of the button to select the month.\n",
    "    - num_month (int): Number of months from the current month to navigate to get to desired month. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    for _ in range(num_month):\n",
    "        browser.find_element('xpath', xpath_button).click()\n",
    "\n",
    "def select_date_range(browser, from_day, to_day):\n",
    "    \"\"\"\n",
    "    Selects a date range on a calendar.\n",
    "\n",
    "    Parameters:\n",
    "    - browser: The browser instance to interact with.\n",
    "    - from_day (str): The start day of the date range in 'DD' format.\n",
    "    - to_day (str): The end day of the date range in 'DD' format.\n",
    "    \"\"\"\n",
    "\n",
    "    path = '//div[@id=\"calendar-searchboxdatepicker\"]//table[@class=\"eb03f3f27f\"]//tbody//td[@class=\"b80d5adb18\"]//span[@class=\"cf06f772fa\"]'\n",
    "    dates = browser.find_elements('xpath', path)\n",
    "\n",
    "    for date in dates:\n",
    "        date_value = date.get_attribute(\"data-date\")\n",
    "        if date_value == f\"2024-06-{from_day}\":\n",
    "            date.click()\n",
    "        if date_value == f\"2024-06-{to_day}\":\n",
    "            date.click()\n",
    "            break\n",
    "\n",
    "def calendar_days(browser, from_day, to_day):\n",
    "    \"\"\"\n",
    "    Main function to select desired days of the calendar.\n",
    "\n",
    "    Parameters:\n",
    "    - browser : The browser instance to interact with.\n",
    "    - from_day (str): The start day of the date range in 'DD' format. \n",
    "    - to_day (str): The end day of the date range in 'DD' format. \n",
    "    \"\"\"\n",
    "\n",
    "    num_clicks = 1\n",
    "    arrow_xpath = '/html/body/div[3]/div[2]/div/form/div[1]/div[2]/div/div[2]/div/nav/div[2]/div/div[1]/button/span/span'\n",
    "    click_element(browser, arrow_xpath, num_clicks)\n",
    "\n",
    "    # change num_month depending on how far the month is from today\n",
    "    num_month = 3 \n",
    "    month_xpath = '/html/body/div[3]/div[2]/div/form/div[1]/div[2]/div/div[2]/div/nav/div[2]/div/div[1]/button[2]'\n",
    "    select_month(browser, month_xpath, num_month)\n",
    "\n",
    "    select_date_range(browser, from_day, to_day)\n",
    "\n",
    "\"\"\"\n",
    "Example:\n",
    "main(browser, from_day='15', to_day='20') \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main calendar days function\n",
    "calendar_days(browser, '12', '16') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XPath to search \n",
    "my_xpath='/html/body/div[3]/div[2]/div/form/div[1]/div[4]/button/span'\n",
    "\n",
    "check_obscures(browser,my_xpath , type='xpath')\n",
    "check_and_click(browser,my_xpath , type='xpath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Pages to click through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_pages(browser):\n",
    "    '''\n",
    "    Get the number of pages. \n",
    "    '''\n",
    "    a = browser.find_elements('xpath',\n",
    "        '//div[@class=\"ab95b25344\"]')\n",
    "    return a[0].text.split(\"\\n\")[-1]\n",
    "    #return(int(a[-1].text))\n",
    "\n",
    "\n",
    "pages = get_number_pages(browser)\n",
    "\n",
    "print(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPaths to search through the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAGES XPATH \n",
    "css_pages = 'div.b16a89683f:nth-child(3) > button:nth-child(1) > span:nth-child(1) > span:nth-child(1)'\n",
    "\n",
    "#XPATHS\n",
    "sections = browser.find_elements('xpath', '//div[@class=\"c066246e13\"]')\n",
    "hotels_xpath = './/div[@class=\"f6431b446c a15b38c233\"]'\n",
    "ratings_xpath = './/div[@class=\"a3b8729ab1 d86cee9b25\"]'\n",
    "prices_xpath = './/span[@class=\"f6431b446c fbfd7c1165 e84eb96b1f\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT HOTEL NAMES, RATINGS, & PRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotel_information_scraping(browser, hotels_xpath, ratings_xpath, prices_xpath, css_pages, pages): \n",
    "    \"\"\" \n",
    "    Scrapes hotel information from a series of pages using Selenium. \n",
    "\n",
    "    Parameters: \n",
    "        - browser (Webdriver): The Selenium Webdriver instance. \n",
    "        - hotels_xpath (str): XPath to locate the hotel name element within each section of hotel options. \n",
    "        - ratings_xpath (str): XPath to locate the rating element within each section of hotel options. \n",
    "        - prices_xpath (str): XPath to locate the price element within each section of hotel options. \n",
    "        - css_pages (str): CSS Selector to locate the element for switiching pages. \n",
    "        - pages (int): The total number of pages to scrape found using function get_number_pages(). \n",
    "\n",
    "    Returns: \n",
    "        - pd.Dataframe: A Dataframe containing the scraped hotel information. \n",
    "\n",
    "    Notes: \n",
    "        - The function prints the hotel information for each section on each page. \n",
    "        - If using for a different website, the xpath for the Section & Hotel URLs need to be replaced with the correct path. \n",
    "\n",
    "    Example: \n",
    "       '''python:  df_hotel = scrape_hotel_data(browser, hotels,xpath, ratings_xpath, prics_xpath, css_pages, pages)'''\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Lists to store the extracted information\n",
    "    hotels_list = []\n",
    "    ratings_list = []\n",
    "    prices_list = []\n",
    "    url_list = []\n",
    "    \n",
    "\n",
    "    for page in range(int(pages)+1): \n",
    "        #Print page that it is in \n",
    "        print(f'Page: {page + 1}')\n",
    "        sections = browser.find_elements('xpath', '//div[@class=\"c066246e13\"]')\n",
    "        for hotel in sections:\n",
    "            try:\n",
    "                hotel_name = hotel.find_element('xpath', hotels_xpath).text\n",
    "\n",
    "            except NoSuchElementException: \n",
    "                print(\"Element could not be found. Printing page source:\", browser.page_source)\n",
    "                continue # Skip to the next iteration if the hotel section was not found \n",
    "\n",
    "            except StaleElementReferenceException: \n",
    "                    print(\"StaleElementReferenceException. Retrying...\")\n",
    "                    #Retry finding the element by waiting for it be present \n",
    "                    wait = WebDriverWait(browser, 10)\n",
    "\n",
    "                    try: \n",
    "                        hotel_name = wait.until(EC.presence_of_element_located(('xpath', hotels_xpath))).text \n",
    "                    except NoSuchElementException: \n",
    "                        print(\"Element still cannot be found. Skipping to next Hotel.\")\n",
    "                        continue\n",
    "                \n",
    "            try:\n",
    "                rating = hotel.find_element('xpath', ratings_xpath).text\n",
    "            except:\n",
    "                rating = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                price = hotel.find_element('xpath', prices_xpath).text\n",
    "            except:\n",
    "                price = \"N/A\"  # I don't think i need an N/A for the price cause there should always be a price \n",
    "\n",
    "            try: \n",
    "                url = hotel.find_element('xpath', './/a[@href]')\n",
    "                hotel_url= url.get_attribute('href')\n",
    "            except:\n",
    "                hotel_url = \"N/A\"\n",
    "        \n",
    "            # Print the hotel information as it runs through each section\n",
    "            print(\"Hotel Name:\", hotel_name)\n",
    "            print(\"Rating:\", rating)\n",
    "            print(\"Price\", price)\n",
    "            print(\"Hotel URL\", hotel_url)\n",
    "            print(\"\\n\")\n",
    "\n",
    "            # Append the information to their respective lists\n",
    "            hotels_list.append(hotel_name)\n",
    "            ratings_list.append(rating)\n",
    "            prices_list.append(price)\n",
    "            url_list.append(hotel_url)\n",
    "\n",
    "\n",
    "\n",
    "            # Switch page with CSS Selector\n",
    "        switch_pages = browser.find_element('css selector', css_pages).click()\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "    # Create a dictonary to store all list together\n",
    "    dict = { 'Hotel Name': hotels_list, 'Rating': ratings_list, 'Price': prices_list, 'Hotel URL': url_list }\n",
    "\n",
    "    # Create a dataframe with the dictionary\n",
    "    df = pd.DataFrame(dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running hotel scrape function. Repeat for all desired cities needed to scrape. \n",
    "df = hotel_information_scraping(browser, hotels_xpath, ratings_xpath, prices_xpath, css_pages, pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT DESCRIPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_info(url):\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts and returns the description text from a specified URL. \n",
    "    \n",
    "    Parameters: \n",
    "        - url (str): The URL of the webpage to extract the description text from. \n",
    "\n",
    "    Returns: \n",
    "        - str or None: The extracted description text that is found, otherwise a None. \n",
    "\n",
    "    Raises: \n",
    "        - request.exceptions.RequestException: In place in case there is a potential issue with the HTTP request.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        req = requests.get(url, headers=headers, timeout=60)\n",
    "        req.raise_for_status() \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Can't process the {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(req.text, 'html.parser')\n",
    "    description_text = soup.find('p', class_='a53cbfa6de b3efd73f69')\n",
    "\n",
    "    if description_text:\n",
    "        return description_text.get_text(strip=True)\n",
    "    else:\n",
    "        print(\"Description could not be found\")\n",
    "        return None\n",
    "\n",
    "# Create an empty list to store the descriptions\n",
    "descriptions_list = []\n",
    "\n",
    "# Iterate through the URLs and scrape descriptions sequentially\n",
    "for url in df['Hotel URL']:\n",
    "    description = description_info(url)\n",
    "    descriptions_list.append(description)\n",
    "\n",
    "# Assign the descriptions to the 'Descriptions' column in the DataFrame\n",
    "df['Descriptions'] = descriptions_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = df.to_csv('Alicante_festival.csv') # Change name of csv as desired for each week that is being scraped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
